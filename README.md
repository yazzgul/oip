# Курс: Основы информационного поиска

# Студент: Язгуль Хасаншина, 11-203

## Задание 1 (директория task-1): Веб-краулер (сбор текстовых страниц)

### Описание
Скрипт на Python загружает не менее 100 веб-страниц из предварительно подготовленного списка URL (файл `urls.txt`). Каждая страница сохраняется в отдельный текстовый файл вместе с HTML-разметкой. Создаётся индексный файл `index.txt` с соответствием номеров файлов и исходных ссылок.

**Цель задания**: получить корпус русскоязычных веб-страниц для дальнейшего анализа.

### Требования к окружению
- Python 3.7 или выше
- Библиотека `requests` (устанавливается через pip)

### Установка и запуск

1. Клонируйте репозиторий:
   ```bash
   git clone https://github.com/ваш-username/ваш-репозиторий.git
   cd ваш-репозиторий/task01_web_crawler
